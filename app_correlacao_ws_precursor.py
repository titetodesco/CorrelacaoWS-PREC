import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from io import BytesIO

st.set_page_config(page_title="Mapa Precursores ‚Üî Weak Signals", layout="wide")

st.title("üîé Mapeamento Tr√≠plice: Precursores (HTO) ‚Üî Weak Signals")
st.caption("Carrega a planilha consolidada `mapeamento_triplice_WS_PREC.xlsx` e permite explorar rela√ß√µes por precursor.")

# ======= Sidebar / Entrada de dados =======
st.sidebar.header("üìÑ Dados")
#uploaded = st.sidebar.file_uploader("Selecione o arquivo `mapeamento_triplice_WS_PREC.xlsx`", type=["xlsx"])

# URLs dos arquivos no GitHub
uploaded = "https://raw.githubusercontent.com/titetodesco/CorrelacaoWS-PREC/main/mapeamento_triplce_WS_PREC.xlsx"
if uploaded is None:
    st.info("Fa√ßa upload da planilha consolidada para iniciar.")
    st.stop()

TRIPLE_SHEET = "Triple_map"

try:
    # L√™ somente a aba Triple_map
    df = pd.read_excel(uploaded, sheet_name=TRIPLE_SHEET)
except ValueError as e:
    st.error(f"N√£o encontrei a aba '{TRIPLE_SHEET}' no arquivo enviado. "
             f"Abra seu Excel e confirme que existe exatamente uma aba com este nome.")
    st.stop()
except Exception as e:
    st.error(f"Erro ao ler a planilha: {e}")
    st.stop()

# Valida√ß√£o m√≠nima das colunas necess√°rias
required_cols = {"Report", "Text", "Top_WS", "Top_Precursores"}
missing = required_cols - set(df.columns)
if missing:
    st.error(f"A aba '{TRIPLE_SHEET}' n√£o cont√©m as colunas m√≠nimas {missing}. "
             f"Verifique a gera√ß√£o do arquivo.")
    st.stop()

# Garante colunas opcionais para n√£o quebrar o resto do app
for c in ["Unit", "Page", "Evidencia"]:
    if c not in df.columns:
        df[c] = np.nan

st.success(f"Aba '{TRIPLE_SHEET}' carregada com {len(df):,} linhas.")

# Esperado: colunas criadas no bloco 'Triple_map' (ou equivalente)
# ["Report","Unit","Page","Text","Top_WS","Top_Precursores","Evidencia"]
expected = {"Report","Text","Top_WS","Top_Precursores"}
if not expected.issubset(set(df.columns)):
    st.error(f"A planilha n√£o cont√©m as colunas m√≠nimas {expected}. Verifique o arquivo gerado.")
    st.stop()

# ======= Helpers p/ parsing =======
import re

def parse_ws_list(s: str):
    if not isinstance(s, str) or not s.strip():
        return []
    out = []
    for part in s.split(";"):
        t = part.strip()
        if not t:
            continue
        m = re.match(r"(.+?)\s*\(([-+]?\d*\.?\d+)\)\s*$", t)
        if m:
            name = m.group(1).strip()
            try:
                score = float(m.group(2))
            except:
                score = np.nan
            out.append((name, score))
        else:
            out.append((t, np.nan))
    return out

def parse_prec_list(s: str):
    if not isinstance(s, str) or not s.strip():
        return []
    out = []
    for part in s.split(";"):
        t = part.strip()
        if not t:
            continue
        m = re.match(r"(.+?)\s*\[([^\]]+)\]\s*\(([-+]?\d*\.?\d+)\)\s*$", t)
        if m:
            name = m.group(1).strip()
            hto  = m.group(2).strip()
            try:
                score = float(m.group(3))
            except:
                score = np.nan
            out.append((name, hto, score))
        else:
            m2 = re.match(r"(.+?)\s*\[([^\]]+)\]\s*(?:\(([-+]?\d*\.?\d+)\))?\s*$", t)
            if m2:
                name = m2.group(1).strip()
                hto  = m2.group(2).strip()
                score = float(m2.group(3)) if m2.group(3) else np.nan
                out.append((name, hto, score))
            else:
                out.append((t, "", np.nan))
    return out

# Explodir pares (cartesiano) por linha do texto
def explode_pairs(df_in: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for _, r in df_in.iterrows():
        ws_list = parse_ws_list(r.get("Top_WS",""))
        prec_list = parse_prec_list(r.get("Top_Precursores",""))
        if not ws_list or not prec_list:
            continue
        for ws_name, ws_sim in ws_list:
            for prec_name, hto, prec_sim in prec_list:
                rows.append({
                    "Report": r.get("Report"),
                    "Unit": r.get("Unit"),
                    "Page": r.get("Page"),
                    "Text": r.get("Text"),
                    "WeakSignal": ws_name,
                    "WS_Similarity": ws_sim,
                    "Precursor": prec_name,
                    "HTO": hto,
                    "Precursor_Similarity": prec_sim,
                    "Evidencia": r.get("Evidencia","")
                })
    return pd.DataFrame(rows)

@st.cache_data(show_spinner=False)
def build_pairs(df_in):
    return explode_pairs(df_in)

pairs = build_pairs(df)

if pairs.empty:
    st.warning("N√£o foram encontrados pares (Top_WS x Top_Precursores). Verifique a planilha.")
    st.stop()

# ======= Filtros globais =======
st.sidebar.header("üéöÔ∏è Filtros")
only_evidence = st.sidebar.checkbox("Somente evid√™ncia dupla (texto com WS e Precursor)", value=False)
min_ws = st.sidebar.slider("Similaridade m√≠nima (Weak Signal)", 0.0, 0.9, 0.50, 0.05)
min_prec = st.sidebar.slider("Similaridade m√≠nima (Precursor)", 0.0, 0.9, 0.50, 0.05)

# üîΩ NOVO: Filtro por frequ√™ncia m√≠nima do trio (HTO, Precursor, WeakSignal)
min_freq = st.sidebar.number_input(
    "Frequ√™ncia m√≠nima (n¬∫ de textos/relat√≥rios)",
    min_value=1, max_value=100, value=1, step=1,
    help="S√≥ mant√©m pares (HTO, Precursor, WeakSignal) com pelo menos esse n¬∫ de ocorr√™ncias."
)

st.sidebar.header("üßÆ R√≥tulos de aresta")
show_edge_labels = st.sidebar.checkbox("Mostrar r√≥tulo de for√ßa na aresta", value=False)
edge_label_metric = st.sidebar.selectbox(
    "M√©trica do r√≥tulo",
    ["Frequencia", "WS_Sim_med", "WS_Sim_max", "Prec_Sim_med", "Prec_Sim_max"],
    index=0,
    help="Qual valor imprimir na aresta Precursor ‚Üî WeakSignal"
)

df_filt = pairs.copy()
if only_evidence:
    df_filt = df_filt[df_filt["Evidencia"].astype(str).str.len() > 0]
df_filt = df_filt[df_filt["WS_Similarity"].fillna(0) >= float(min_ws)]
df_filt = df_filt[df_filt["Precursor_Similarity"].fillna(0) >= float(min_prec)]

# Depois de aplicar filtros globais em df_filt, monte um preview com frequ√™ncia
preview = (df_filt
    .groupby(["HTO","Precursor","WeakSignal"], as_index=False)
    .agg(Frequencia=("Text","nunique"))
)
preview = preview[preview["Frequencia"] >= int(min_freq)]

opts = (preview[["HTO","Precursor"]]
        .drop_duplicates()
        .sort_values(["HTO","Precursor"]))
opts["label"] = opts["HTO"] + " ‚Äî " + opts["Precursor"]


# ======= Seletor de precursor =======
opts = (df_filt[["HTO","Precursor"]]
        .drop_duplicates()
        .sort_values(["HTO","Precursor"]))
opts["label"] = opts["HTO"] + " ‚Äî " + opts["Precursor"]

if opts.empty:
    st.warning("Nenhum (HTO, Precursor) dispon√≠vel com os filtros atuais.")
    st.stop()

choice = st.selectbox("Escolha o Precursor (HTO ‚Äî Precursor)", options=opts["label"].tolist(), index=0)
sel_ht, sel_prec = choice.split(" ‚Äî ", 1)

# ======= Agrega√ß√£o (para o precursor escolhido) =======
df_prec = (df_filt[(df_filt["HTO"]==sel_ht) & (df_filt["Precursor"]==sel_prec)]
           .groupby(["HTO","Precursor","WeakSignal"], as_index=False)
           .agg(Frequencia=("Text","nunique"),
                WS_Sim_med=("WS_Similarity","mean"),
                WS_Sim_max=("WS_Similarity","max"),
                Prec_Sim_med=("Precursor_Similarity","mean"),
                Prec_Sim_max=("Precursor_Similarity","max"),
                Reports=("Report", lambda s: ", ".join(sorted(set(map(str,s)))[:10])))
           .sort_values(["Frequencia","WS_Sim_max","Prec_Sim_max"], ascending=[False,False,False])
)

# üîΩ NOVO: mant√©m apenas sinais com Frequencia >= min_freq
df_prec = df_prec[df_prec["Frequencia"] >= int(min_freq)]

col1, col2 = st.columns([2,1], vertical_alignment="top")
with col1:
    st.subheader(f"Sinais fracos associados a: {sel_ht} ‚Äî {sel_prec}")
    if df_prec.empty:
        st.info("Sem sinais para este precursor com os filtros atuais.")
    else:
        # Gr√°fico de barras (frequ√™ncia)
        fig = px.bar(
            df_prec.sort_values("Frequencia", ascending=True),
            x="Frequencia", y="WeakSignal", orientation="h",
            title="Frequ√™ncia por Weak Signal",
            hover_data=["WS_Sim_med","WS_Sim_max","Prec_Sim_med","Prec_Sim_max"]
        )
        st.plotly_chart(fig, use_container_width=True, theme="streamlit")

with col2:
    # Pequeno heatmap opcional (similaridade m√©dia WS x frequ√™ncia)
    if not df_prec.empty:
        top_n = st.number_input("Top-N para Heatmap", 5, 50, 15, 1)
        df_hm = df_prec.head(int(top_n)).copy()
        if not df_hm.empty:
            st.caption("Heatmap (freq x sim. m√©dia WS)")
            # normalizar para visual
            df_hm["WS_Sim_med_norm"] = (df_hm["WS_Sim_med"] - df_hm["WS_Sim_med"].min()) / max(1e-9, (df_hm["WS_Sim_med"].max() - df_hm["WS_Sim_med"].min()))
            fig2 = px.imshow(
                np.array([df_hm["Frequencia"].tolist(), (100*df_hm["WS_Sim_med"]).tolist()]),
                labels=dict(x="WeakSignal (Top-N)", y="M√©trica", color="Valor"),
                x=df_hm["WeakSignal"].tolist(),
                y=["Frequ√™ncia", "WS_sim (%)"]
            )
            st.plotly_chart(fig2, use_container_width=True, theme="streamlit")

st.sidebar.header("‚öôÔ∏è F√≠sica do grafo")
gravity = st.sidebar.slider("Gravidade (negativo = repulsa)", -20000, -500, -8000, 500)
spring_length = st.sidebar.slider("Comprimento da mola", 60, 300, 140, 10)
spring_const = st.sidebar.slider("Constante da mola", 0.01, 0.20, 0.03, 0.01)
damping = st.sidebar.slider("Damping", 0.60, 0.95, 0.85, 0.01)


st.divider()
st.subheader("Tabela detalhada (pares)")

# -----------------------------------------------------------
# ABA "Grafo" ‚Äî Rede Precursor (HTO) ‚Üî WeakSignal (com filtros)
# -----------------------------------------------------------
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components

st.subheader("üï∏Ô∏è Grafo: Precursores (HTO) ‚Üî Weak Signals")

# 1) Agregar pares com os filtros globais aplicados
edges_df = (df_filt
    .groupby(["HTO","Precursor","WeakSignal"], as_index=False)
    .agg(Frequencia=("Text","nunique"),
         WS_Sim_med=("WS_Similarity","mean"),
         WS_Sim_max=("WS_Similarity","max"),
         Prec_Sim_med=("Precursor_Similarity","mean"),
         Prec_Sim_max=("Precursor_Similarity","max"),
         Reports=("Report", lambda s: ", ".join(sorted(set(map(str,s)))[:8])))
)

# 2) Aplicar corte pela frequ√™ncia m√≠nima
edges_df = edges_df[edges_df["Frequencia"] >= int(min_freq)].copy()

if edges_df.empty:
    st.info("Nenhuma aresta atende aos filtros atuais (verifique frequ√™ncia m√≠nima e limiares de similaridade).")
else:
    # 3) Construir grafo bipartido
    G = nx.Graph()

    # paleta por HTO (ajuste se desejar)
    HTO_COLORS = {
        "Humano": "#1f78b4",
        "T√©cnico": "#33a02c",
        "Tecnico": "#33a02c",     # caso venha sem acento
        "Organizacional": "#e31a1c",
        "Organizacinal": "#e31a1c"  # toler√¢ncia a typo comum
    }
    WS_COLOR = "#ff7f00"

    # graus acumulados para dimensionar n√≥s
    freq_by_prec = edges_df.groupby(["HTO","Precursor"])["Frequencia"].sum().to_dict()
    freq_by_ws   = edges_df.groupby("WeakSignal")["Frequencia"].sum().to_dict()

    # 3a) N√≥s de Precursor (com HTO)
    for (hto, prec), freq_sum in freq_by_prec.items():
        color = HTO_COLORS.get(str(hto), "#6a3d9a")
        size = 10 + 3 * np.log1p(freq_sum)   # escala suave pelo log da frequ√™ncia somada
        G.add_node(
            f"P::{hto}::{prec}",
            label=f"{prec} [{hto}]",
            title=f"<b>Precursor</b>: {prec}<br><b>HTO</b>: {hto}<br><b>Freq total</b>: {freq_sum}",
            color=color, shape="dot", size=float(size), group=str(hto), node_type="precursor"
        )

    # 3b) N√≥s de WeakSignal
    for ws, freq_sum in freq_by_ws.items():
        size = 8 + 3 * np.log1p(freq_sum)
        G.add_node(
            f"WS::{ws}",
            label=ws,
            title=f"<b>Weak Signal</b>: {ws}<br><b>Freq total</b>: {freq_sum}",
            color=WS_COLOR, shape="dot", size=float(size), group="WS", node_type="ws"
        )

    # 3c) Arestas (peso = frequ√™ncia; tooltip com stats)
    for _, r in edges_df.iterrows():
        hto, prec, ws = r["HTO"], r["Precursor"], r["WeakSignal"]
        freq = int(r["Frequencia"])
        ws_med, ws_max = float(r["WS_Sim_med"]), float(r["WS_Sim_max"])
        pr_med, pr_max = float(r["Prec_Sim_med"]), float(r["Prec_Sim_max"])
    
        # defina SEMPRE o t√≠tulo antes de usar
        title = (
            f"<b>{prec} [{hto}]</b> ‚Üî <b>{ws}</b><br>"
            f"Frequ√™ncia: {freq}<br>"
            f"WS sim (m√©dia/m√°x): {ws_med:.2f} / {ws_max:.2f}<br>"
            f"Prec sim (m√©dia/m√°x): {pr_med:.2f} / {pr_max:.2f}<br>"
            f"Relat√≥rios: {r.get('Reports', '')}"
        )
    
        # r√≥tulo opcional na aresta
        edge_kwargs = {
            "value": freq,
            "title": title,
            "width": float(1 + np.log1p(freq)),
        }
        if show_edge_labels:
            val = r[edge_label_metric]
            edge_kwargs["label"] = (
                f"{val:.2f}" if isinstance(val, (float, np.floating)) else str(int(val))
                if isinstance(val, (int, np.integer)) else str(val)
            )
    
        G.add_edge(f"P::{hto}::{prec}", f"WS::{ws}", **edge_kwargs)

    # 4) Renderizar com PyVis e embutir no Streamlit
    net = Network(height="700px", width="100%", bgcolor="#ffffff", font_color="#222222", directed=False, notebook=False)
    net.barnes_hut(gravity=-2000, central_gravity=0.2, spring_length=120, spring_strength=0.045, damping=0.9)
    net.from_nx(G)

    # habilita physics e intera√ß√£o
    import json

    options = {
        "nodes": {
            "borderWidth": 1,
            "shadow": False
        },
        "edges": {
            "smooth": {"type": "dynamic", "roundness": 0.5},
            "color": {"opacity": 0.7}
        },
        "physics": {
            "enabled": True,
            "stabilization": {"iterations": 150},
            "barnesHut": {
                # use os sliders da sidebar aqui:
                "gravitationalConstant": int(gravity),
                "springLength": int(spring_length),
                "springConstant": float(spring_const),
                "damping": float(damping)
            }
        },
        "interaction": {
            "hover": True,
            "navigationButtons": True,
            "keyboard": True,
            "selectable": True,
            "multiselect": True,
            "zoomView": True
        }
    }
    
    net.set_options(json.dumps(options))


    
    # Renderizar
    #net.save_graph("graph.html")
    #st.components.v1.html(open("graph.html").read(), height=800, scrolling=True)

    # Renderizar apenas uma vez
    html_path = "graph_prec_ws.html"
    net.save_graph(html_path)
    with open(html_path, "r", encoding="utf-8") as f:
        html = f.read()
    components.html(html, height=720, scrolling=True)


    # 5) Download das tabelas (arestas e n√≥s)
    st.markdown("**Downloads (dados do grafo filtrado):**")
    colA, colB = st.columns(2)
    with colA:
        st.download_button(
            "üì• Arestas (CSV)",
            data=edges_df.to_csv(index=False).encode("utf-8"),
            file_name="edges_precursor_ws.csv",
            mime="text/csv"
        )
    # construir tabela de n√≥s para download
    nodes_rows = []
    for nid, attrs in G.nodes(data=True):
        nodes_rows.append({
            "node_id": nid,
            "label": attrs.get("label",""),
            "group": attrs.get("group",""),
            "node_type": attrs.get("node_type",""),
            "size": attrs.get("size",""),
            "color": attrs.get("color","")
        })
    nodes_df = pd.DataFrame(nodes_rows)
    with colB:
        st.download_button(
            "üì• N√≥s (CSV)",
            data=nodes_df.to_csv(index=False).encode("utf-8"),
            file_name="nodes_precursor_ws.csv",
            mime="text/csv"
        )

    st.caption("Dica: ajuste os filtros (similaridade e frequ√™ncia) na barra lateral para controlar a densidade do grafo.")

# =====================================
# 2) Grafo por Relat√≥rio (novo recurso)
# =====================================

st.subheader("üîé Grafo por Relat√≥rio")

# a) Sele√ß√£o do relat√≥rio
reports = df["Report"].dropna().unique()
selected_report = st.selectbox("Selecione um relat√≥rio", reports)

if selected_report:
    df_report = df[df["Report"] == selected_report]

    # b) Construir grafo
    net_report = Network(height="600px", width="100%", directed=False)

    # N√≥ central = relat√≥rio
    net_report.add_node(
        selected_report,
        label=selected_report,
        shape="box",
        color="#FFD700",
        title=f"Relat√≥rio: {selected_report}"
    )

    # c) Adicionar Precursores e WS ligados ao relat√≥rio
    for _, row in df_report.iterrows():
        precursor = row["Top_Precursores"]
        ws = row["Top_WS"]
        freq = row.get("Freq", 1)  # caso tenha coluna de frequ√™ncia

        # Precursor ‚Üî Relat√≥rio
        net_report.add_node(precursor, label=precursor, color="#87CEFA", title="Precursor")
        net_report.add_edge(selected_report, precursor, value=freq, title=f"Frequ√™ncia: {freq}")

        # Weak Signal ‚Üî Relat√≥rio
        net_report.add_node(ws, label=ws, color="#90EE90", title="Weak Signal")
        net_report.add_edge(selected_report, ws, value=freq, title=f"Frequ√™ncia: {freq}")

        # Opcional: liga√ß√£o Precursor ‚Üî Weak Signal
        net_report.add_edge(precursor, ws, value=freq, title=f"Correla√ß√£o no relat√≥rio")

    # Renderizar
    net_report.set_options("""{
        "nodes": {"font": {"size": 18}},
        "edges": {"color": {"inherit": "from"}, "smooth": false}
    }""")

    html_path = "graph_report.html"
    net_report.save_graph(html_path)
    st.components.v1.html(open(html_path).read(), height=800, scrolling=True)

st.dataframe(df_prec, use_container_width=True)

# Download do recorte atual (precursor)
def to_excel_bytes(df_in: pd.DataFrame) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="xlsxwriter") as writer:
        df_in.to_excel(writer, sheet_name="dados", index=False)
    bio.seek(0)
    return bio.read()

st.download_button(
    "üì• Baixar Excel (precursor filtrado)",
    data=to_excel_bytes(df_prec),
    file_name=f"{sel_ht}_{sel_prec}_weak_signals.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)

st.caption("Dica: ajuste os sliders de similaridade para reduzir ru√≠do ou focar em correla√ß√µes mais fortes.")
